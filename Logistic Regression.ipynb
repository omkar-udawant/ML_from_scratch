{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Logistic regression is used to describe data and explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables.\n",
    "\n",
    "Logistic regression models the probablity that each of its input belongs to a particular category i.e. given a set of inputs X, we want to assign them to one of two possible categories (0 or 1)\n",
    "\n",
    "\n",
    "A function takes some input and gives some output. To get probablities logistic regression uses a function that gives values between 0 and 1 for all values of input X, this function is called as `Sigmoid function` represented in following image.\n",
    "\n",
    "\n",
    "![sigmoid curve](https://miro.medium.com/max/700/1*HXCBO-Wx5XhuY_OwMl0Phw.png)\n",
    "\n",
    "Sigmoid function:\n",
    "![sigmoid function](https://miro.medium.com/max/292/1*p4hYc2VwJqoLWwl_mV0Vjw.png)\n",
    "\n",
    "\n",
    "Functions have parameters/weights (represented by theta here) and we want to find the best values for them. To start we pick random values and we need a way to measure how well the algorithm performs using those random weights. That measure is computed using the loss function, \n",
    "\n",
    "\n",
    "Loss function:\n",
    "![](https://miro.medium.com/max/700/1*FdxEs8Iv_43Q8calTCjnow.png)\n",
    "\n",
    "\n",
    "Our goal is to minimize this loss function, which is done by increasing or decreasing the weights. And this is done by the derivative of the loss function with respect to each weight. It tells us how loss would change if we modified the parameters which is nothing but Gradient descent after which we get the updated weights by sbtracting this derivative times the learning rate.\n",
    "\n",
    "Partial derivative:\n",
    "![gradient descent](https://miro.medium.com/max/536/1*gobKgGbRWDAoVFAan_HjxQ.png)\n",
    "\n",
    "\n",
    "This process is repeated untill we find the optimal solution.\n",
    "\n",
    "For prediction, from the sigmoid function we get the probability that some input x belongs to class 1. Let’s take all probabilities ≥ 0.5 = class 1 and all probabilities < 0 = class 0. This threshold should be defined depending on the business problem we were working.\n",
    "\n",
    "Converting all this theory to code and we get, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \"\"\"\n",
    "    Logistic regression from scratch using numpy\n",
    "    \"\"\"\n",
    "    def __init__(self, lr=0.01, num_iter=100000, fit_intercept=True, theta=0, verbose=False):\n",
    "        self.lr = lr\n",
    "        self.num_iter = num_iter\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.theta = theta\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def add_intercept(self, X):\n",
    "        \"\"\"\n",
    "        This function computes the intercept Twhich is the expected mean value of Y when all X=0.\n",
    "        \"\"\"\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.concatenate((intercept, X), axis=1)\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"\n",
    "        This function gives the probablities between 0 and 1\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def loss(self, h, y):\n",
    "        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if self.fit_intercept:\n",
    "            X = self.add_intercept(X)\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "        \n",
    "        for i in range(self.num_iter):\n",
    "            z = np.dot(X, self.theta)\n",
    "            h = self.sigmoid(z)\n",
    "            gradient = np.dot(X.T, (h - y)) / y.size\n",
    "            self.theta -= self.lr * gradient\n",
    "            \n",
    "            if(self.verbose == True and i % 10000 == 0):\n",
    "                z = np.dot(X, self.theta)\n",
    "                h = self.sigmoid(z)\n",
    "                print(f'loss: {self.loss(h, y)} \\t')\n",
    "    \n",
    "    def predict_probablities(self, X):\n",
    "        if self.fit_intercept:\n",
    "            X = self.add_intercept(X)\n",
    "    \n",
    "        return self.sigmoid(np.dot(X, self.theta))\n",
    "    \n",
    "    def predict(self, X, threshold):\n",
    "        return self.predict_probablities(X) >= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(lr=0.1, num_iter=300000)\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, :2]\n",
    "y = (iris.target != 0) * 1\n",
    "\n",
    "X_train, y_train = X[:100], y[:100]\n",
    "X_test, y_test = X[100:], y[100:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10 µs, sys: 1 µs, total: 11 µs\n",
      "Wall time: 21.7 µs\n"
     ]
    }
   ],
   "source": [
    "%time \n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(X_test, threshold=0.5)\n",
    "(preds == y_test).mean()  #Accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
